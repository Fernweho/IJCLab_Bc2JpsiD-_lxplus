{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#SELECTION DEFINITION\n",
    "\n",
    "THE_BIG_SELECTION = (\n",
    "    \"((Dst_M - D0_M) > 143) & ((Dst_M - D0_M) < 148) & \"\n",
    "    \"(K_IPCHI2_OWNPV > 3) & (pi_IPCHI2_OWNPV > 3) & \"\n",
    "    \"(K_TRACK_CHI2NDOF < 1.7) & (pi_TRACK_CHI2NDOF < 1.7) & \"\n",
    "    \"(K_PT > 250) & (pi_PT > 250) & \"\n",
    "    \"(K_hasRich == 1) & (pi_hasRich == 1) & \"\n",
    "    \"(mp_TRACK_CHI2NDOF < 2) & \"\n",
    "    \"(mm_TRACK_CHI2NDOF < 2) & \"\n",
    "    \"(mp_isMuon == 1) & (mm_isMuon == 1) & \"\n",
    "    \"(mp_PIDmu > 0) & (mm_PIDmu > 0) & \"\n",
    "    \"(mp_PT > 700) & (mm_PT > 700) & \"\n",
    "    \"(mp_IPCHI2_OWNPV > 2) & (mm_IPCHI2_OWNPV > 2) & \"\n",
    "    \"(Jpsi_M < 3140) & (Jpsi_M > 3040) & \"\n",
    "    \"(pi_soft_PT > 250) & \"\n",
    "    \"(Bc_ENDVERTEX_CHI2 < 4.2) & \"\n",
    "    \"(Bc_IPCHI2_OWNPV < 10) & \"\n",
    "    \"(Bc_DIRA_OWNPV > 0.99)\"\n",
    ")\n",
    "\n",
    "#IMPORTING LIBRARIES\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier, callback, plot_importance\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import joblib\n",
    "from sklearn.datasets import make_classification\n",
    "# evaluate a logistic regression model using k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "#DATA DOWNLOADING\n",
    "files_simu = [\n",
    "    './simulation/2011_MagDown.root', \n",
    "    './simulation/2015_MagDown.root',\n",
    "    './simulation/2015_MagUp.root',\n",
    "    './simulation/2016_MagDown.root',\n",
    "    './simulation/2016_MagUp.root',\n",
    "    './simulation/2017_MagDown.root',\n",
    "    './simulation/2017_MagUp.root',\n",
    "    './simulation/2018_MagDown.root',\n",
    "    './simulation/2018_MagUp.root'\n",
    "]\n",
    "\n",
    "files_real_data = [\n",
    "    './real_data/2011_MagDown.root', \n",
    "    './real_data/2011_MagUp.root',\n",
    "    './real_data/2012_MagDown.root', \n",
    "    './real_data/2012_MagUp.root',\n",
    "    './real_data/2015_MagDown.root', \n",
    "    './real_data/2015_MagUp.root',\n",
    "    './real_data/2016_MagDown.root', \n",
    "    './real_data/2016_MagUp.root',\n",
    "    './real_data/2017_MagDown.root', \n",
    "    './real_data/2017_MagUp.root',\n",
    "    './real_data/2018_MagDown.root', \n",
    "    './real_data/2018_MagUp.root'\n",
    "]\n",
    "simu_data = uproot.concatenate(files_simu, cut = f\"{THE_BIG_SELECTION} & (Bc_BKGCAT == 0)\", library='pd')\n",
    "real_data = uproot.concatenate(files_real_data, cut = f\"{THE_BIG_SELECTION} & ((Bc_FullDTF_M < 6250.) | (Bc_FullDTF_M > 6300.))\", library='pd')\n",
    "real_data['delta_mass'] = real_data['Dst_M'] - real_data['D0_M']\n",
    "simu_data['delta_mass'] = simu_data['Dst_M'] - simu_data['D0_M']\n",
    "#DEFINING TRAINING FEATURES\n",
    "\n",
    "feature_columns = [\n",
    "\n",
    "    'mp_PT',\n",
    "    'mm_PT',\n",
    "    'Bc_ENDVERTEX_CHI2',\n",
    "    'Bc_IPCHI2_OWNPV',\n",
    "\n",
    "    'Bc_FD_OWNPV',\n",
    "    'Jpsi_FD_ORIVX',\n",
    "    'D0_FD_ORIVX'\n",
    "    ]\n",
    "#PREPARING DATASET\n",
    "# Separate features and labels\n",
    "X_simu = simu_data[feature_columns]\n",
    "y_simu = np.ones(len(simu_data))  # Label: 1 for simulation data\n",
    "\n",
    "X_real = real_data[feature_columns]\n",
    "y_real = np.zeros(len(real_data))  # Label: 0 for real data\n",
    "\n",
    "# Combine features and labels\n",
    "X_all = pd.concat([X_simu, X_real], ignore_index=True)\n",
    "y_all = np.concatenate([y_simu, y_real]) \n",
    "\n",
    "print(f\"SIMULATION set size: {X_simu.shape}\")\n",
    "print(f\"REAL DATA set size: {X_real.shape}\")\n",
    "\n",
    "#TIME FUNCTION\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    else:\n",
    "        elapsed_time = datetime.now() - start_time\n",
    "        thour, temp_sec = divmod(elapsed_time.total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %d hours, %d minutes, and %.2f seconds.' % (int(thour), int(tmin), tsec))\n",
    "\n",
    "#IMPLEMENTING OPTUNA TO FIND THE BEST MODEL\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=76)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20)\n",
    "    }\n",
    "    model_optuna = XGBClassifier(**params, random_state=52, eval_metric=\"logloss\")\n",
    "    model_optuna.fit(X_train, y_train)\n",
    "    preds = model_optuna.predict_proba(X_test)[:, 1]\n",
    "    score = roc_auc_score(y_test, preds)\n",
    "    if not hasattr(objective, 'best_score') or score > objective.best_score:\n",
    "        objective.best_score = score\n",
    "        objective.best_model = model_optuna \n",
    "    return score\n",
    "\n",
    "# Optimize\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "start_time = timer(None)\n",
    "study.optimize(objective, n_trials=400)\n",
    "best_model = objective.best_model\n",
    "joblib.dump(best_model, \"best_optuna_model.pkl\")\n",
    "timer(start_time)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best ROC-AUC Score:\", study.best_value)\n",
    "print(\"Best model saved as 'best_optuna_model.pkl'\")\n",
    "\n",
    "#TRAINING MODEL WITH K-FOLD AND EARLY STOPPING\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "n_folds = 5\n",
    "fold_accuracies = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=11)\n",
    "\n",
    "# Load Optuna-saved model parameters\n",
    "with open(\"best_optuna_model.pkl\", \"rb\") as file:\n",
    "    best_model_params = pickle.load(file)\n",
    "try:\n",
    "    print(type(best_model_params))  # Debugging step\n",
    "    print(best_model_params) \n",
    "    if isinstance(best_model_params, XGBClassifier):\n",
    "        best_model_params = best_model_params.get_params()  # Extract parameters from model\n",
    "    elif not isinstance(best_model_params, dict):\n",
    "        raise TypeError(f\"Unexpected type for best_model_params: {type(best_model_params)}\")\n",
    "\n",
    "    elif not isinstance(best_model_params, dict):\n",
    "        raise TypeError(\"Unexpected type for best_model_params:\", type(best_model_params))\n",
    "\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"ImportError occurred:\", e)\n",
    "\n",
    "# Ensure parameters are valid for XGBClassifier\n",
    "if hasattr(best_model_params, 'get_params'):\n",
    "    best_model_params = best_model_params.get_params()\n",
    "\n",
    "\n",
    "best_model_params.update({\n",
    "    'random_state': 51, \n",
    "    'eval_metric': 'logloss' ,\n",
    "    'early_stopping_rounds': 20 \n",
    "})\n",
    "\n",
    "classif_list = []\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_all, y_all)):\n",
    "    print(f\"Processing Fold {fold + 1}/{n_folds}...\")\n",
    "    # Initialize the model with the best parameters\n",
    "    model_es = XGBClassifier(**best_model_params)\n",
    "    # Split the data using iloc for pandas DataFrame compatibility\n",
    "    X_train, X_val = X_all.iloc[train_index], X_all.iloc[val_index]\n",
    "    y_train, y_val = y_all[train_index], y_all[val_index]\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    \n",
    "\n",
    "    # Fit the model with early stopping\n",
    "    model_es.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    classif_list.append(model_es)\n",
    "    # Evaluate the model on the validation set\n",
    "    y_pred = model_es.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    fold_accuracies.append(acc)\n",
    "    print(f\"Fold {fold + 1} Accuracy: {acc:.4f}\")\n",
    "\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"\\nMean Cross-Validation Accuracy: {mean_accuracy:.4f}\")\n",
    "\n",
    "model_es.save_model(\"best_kfold_early_stopping_model.json\")\n",
    "\n",
    "#ACCURACY\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=92)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classif_list[1].predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "numb_of_classif = 5\n",
    "\n",
    "for i in range (numb_of_classif):\n",
    "\n",
    "    # Combine signal and background predictions for each set\n",
    "    train_sim_signal = classif_list[i].predict_proba(X_train[y_train == 1])[:, 1]\n",
    "    train_real_background = classif_list[i].predict_proba(X_train[y_train == 0])[:, 1]\n",
    "    test_sim_signal = classif_list[i].predict_proba(X_test[y_test == 1])[:, 1]\n",
    "    test_real_background = classif_list[i].predict_proba(X_test[y_test == 0])[:, 1]\n",
    "\n",
    "    # Define bins for histograms\n",
    "    bins = np.linspace(0, 1, 50)\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "    # Histograms for training data\n",
    "    train_sim_hist, _ = np.histogram(train_sim_signal, bins=bins, density=True)\n",
    "    train_real_hist, _ = np.histogram(train_real_background, bins=bins, density=True)\n",
    "\n",
    "    # Histograms for test data (used for alignment)\n",
    "    test_sim_hist, _ = np.histogram(test_sim_signal, bins=bins, density=True)\n",
    "    test_real_hist, _ = np.histogram(test_real_background, bins=bins, density=True)\n",
    "\n",
    "    # Plot histograms for training data\n",
    "    plt.hist(train_sim_signal, bins=bins, alpha=0.5, label=\"Training Simulation (Signal)\", color=\"red\", density=True, histtype=\"stepfilled\")\n",
    "    plt.hist(train_real_background, bins=bins, alpha=0.5, label=\"Training Real (Background)\", color=\"blue\", density=True, histtype=\"stepfilled\")\n",
    "\n",
    "    # Plot test data as dots\n",
    "    plt.plot(bin_centers, test_sim_hist, 'o', label=\"Testing Simulation (Signal)\", color=\"red\", markersize=5)\n",
    "    plt.plot(bin_centers, test_real_hist, 'o', label=\"Testing Real (Background)\", color=\"blue\", markersize=5)\n",
    "\n",
    "    # Labels, legend, and title\n",
    "    plt.xlabel(\"Classifier Response (XGBoost Score)\")\n",
    "    plt.ylabel(\"Arbitrary Units (Log Scale)\")\n",
    "    plt.title(\"Classifier Response for Training and Testing Data\")\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")  # Logarithmic scale for better visualization\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "numb_of_classif = 5\n",
    "\n",
    "for i in range(numb_of_classif):\n",
    "\n",
    "    # Combine signal and background predictions for each set\n",
    "    train_sim_signal = classif_list[i].predict_proba(X_train[y_train == 1])[:, 1]\n",
    "    train_real_background = classif_list[i].predict_proba(X_train[y_train == 0])[:, 1]\n",
    "    test_sim_signal = classif_list[i].predict_proba(X_test[y_test == 1])[:, 1]\n",
    "    test_real_background = classif_list[i].predict_proba(X_test[y_test == 0])[:, 1]\n",
    "\n",
    "    # Define a function to calculate the optimal threshold\n",
    "    def find_optimal_threshold(y_true, y_scores):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "        \n",
    "        # Maximize F1 score (where precision and recall are balanced)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Find the threshold that maximizes the F1 score\n",
    "        optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        return optimal_threshold\n",
    "\n",
    "    # Find optimal threshold based on training data\n",
    "    optimal_threshold = find_optimal_threshold(y_train, classif_list[i].predict_proba(X_train)[:, 1])\n",
    "    print(f\"Optimal Threshold for Classifier {i+1}: {optimal_threshold}\")\n",
    "\n",
    "    # Apply the threshold to filter the training and test sets\n",
    "    train_sim_signal_filtered = train_sim_signal[train_sim_signal > optimal_threshold]\n",
    "    train_real_background_filtered = train_real_background[train_real_background > optimal_threshold]\n",
    "    test_sim_signal_filtered = test_sim_signal[test_sim_signal > optimal_threshold]\n",
    "    test_real_background_filtered = test_real_background[test_real_background > optimal_threshold]\n",
    "\n",
    "    # Define bins for histograms\n",
    "    bins = np.linspace(0, 1, 50)\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "    # Histograms for training data (after applying threshold)\n",
    "    train_sim_hist, _ = np.histogram(train_sim_signal_filtered, bins=bins, density=True)\n",
    "    train_real_hist, _ = np.histogram(train_real_background_filtered, bins=bins, density=True)\n",
    "\n",
    "    # Histograms for test data (used for alignment)\n",
    "    test_sim_hist, _ = np.histogram(test_sim_signal_filtered, bins=bins, density=True)\n",
    "    test_real_hist, _ = np.histogram(test_real_background_filtered, bins=bins, density=True)\n",
    "\n",
    "    # Plot histograms for training data\n",
    "    plt.hist(train_sim_signal_filtered, bins=bins, alpha=0.5, label=\"Training Simulation (Signal)\", color=\"red\", density=True, histtype=\"stepfilled\")\n",
    "    plt.hist(train_real_background_filtered, bins=bins, alpha=0.5, label=\"Training Real (Background)\", color=\"blue\", density=True, histtype=\"stepfilled\")\n",
    "\n",
    "    # Plot test data as dots\n",
    "    plt.plot(bin_centers, test_sim_hist, 'o', label=\"Testing Simulation (Signal)\", color=\"red\", markersize=5)\n",
    "    plt.plot(bin_centers, test_real_hist, 'o', label=\"Testing Real (Background)\", color=\"blue\", markersize=5)\n",
    "\n",
    "    # Labels, legend, and title\n",
    "    plt.xlabel(\"Classifier Response (XGBoost Score)\")\n",
    "    plt.ylabel(\"Arbitrary Units (Log Scale)\")\n",
    "    plt.title(f\"Classifier Response for Training and Testing Data (Classifier {i+1})\")\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")  # Logarithmic scale for better visualization\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Show the plot for this classifier\n",
    "    plt.show()\n",
    "    y_pred = classif_list[1].predict(X_test)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Generate a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine signal and background predictions for each set\n",
    "\n",
    "\n",
    "optimal_threshold = 0.8\n",
    "\n",
    "signal_probabilities = classif_list[i].predict_proba(X_all)[:, 1]\n",
    "X_filtered = X_all[signal_probabilities > optimal_threshold]\n",
    "y_filtered = y_all[signal_probabilities > optimal_threshold]\n",
    "print(f\"Optimal Threshold for Classifier {i+1}: {optimal_threshold}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=92)\n",
    "train_sim_signal = classif_list[i].predict_proba(X_train[y_train == 1])[:, 1]\n",
    "train_real_background = classif_list[i].predict_proba(X_train[y_train == 0])[:, 1]\n",
    "test_sim_signal = classif_list[i].predict_proba(X_test[y_test == 1])[:, 1]\n",
    "test_real_background = classif_list[i].predict_proba(X_test[y_test == 0])[:, 1]\n",
    "\n",
    "# Predict using the current classifier (without threshold)\n",
    "y_pred = classif_list[i].predict(X_test)\n",
    "\n",
    "# Calculate accuracy using thresholded predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy for Classifier {i+1}: {accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report using thresholded predictions\n",
    "print(f\"Classification Report for Classifier {i+1}:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Precision-Recall Curve using probabilities\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, classif_list[i].predict_proba(X_test)[:, 1])\n",
    "\n",
    "# ROC Curve using probabilities\n",
    "y_prob = classif_list[i].predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Apply threshold to predictions for ROC curve (thresholding affects the classification)\n",
    "y_pred_thresholded = (y_prob > optimal_threshold).astype(int)\n",
    "\n",
    "# Plot ROC curve with threshold\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'Receiver Operating Characteristic (Classifier {i+1})')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_prob = model_es.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot feature importance\n",
    "plot_importance(model_es, importance_type='weight')\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "real_data_predictions = classif_list[i].predict_proba(X_real)[:, 1]\n",
    "real_data_filtered = real_data[real_data_predictions > 0.8]\n",
    "\n",
    "print(f\"Not-filtered data size: {real_data.shape[0]}\")\n",
    "print(f\"Filtered data size: {real_data_filtered.shape[0]}\")\n",
    "\n",
    "# --- Signal Shape in Range 6200-6350 from Simulation Data ---\n",
    "signal_range = (simu_data['Bc_FullDTF_M'] > 6250) & (simu_data['Bc_FullDTF_M'] < 6300)\n",
    "simu_signal_in_range = simu_data['Bc_FullDTF_M'][signal_range]\n",
    "\n",
    "# Randomly sample 15 events from the simulation data in the range 6200-6350\n",
    "simu_signal_sampled = np.random.choice(simu_signal_in_range, size=15, replace=False)\n",
    "\n",
    "# --- Plot everything as histograms ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "plt.hist(real_data['Bc_FullDTF_M'], range = (5400, 6900), bins=100, alpha=0.7, color='red', label=\"Original Bc_FullDTF Mass (Real Data)\", histtype='step')\n",
    "plt.hist(real_data_filtered['Bc_FullDTF_M'], range = (5400, 6900), bins=100, alpha=0.7, color='green', label=\"Filtered Bc_FullDTF Mass (Real Data)\", histtype='step')\n",
    "# Plot simulation signal in Bc_FullDTF Mass range (6200-6350) as a histogram\n",
    "# plt.hist(simu_signal_in_range, bins=100, alpha=0.7, color='purple', label=\"Simulation Signal (6200-6350 range, 15 events)\", histtype='step')\n",
    "\n",
    "# Labels, legend, and title\n",
    "plt.xlabel(\"Bc_FullDTF Mass\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.title(\"Bc_FullDTF Mass Distribution of Real Data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "real_data_predictions = classif_list[i].predict_proba(X_real)[:, 1]\n",
    "real_data_filtered = real_data[real_data_predictions > 0.8]\n",
    "\n",
    "print(f\"Not-filtered data size: {real_data.shape[0]}\")\n",
    "print(f\"Filtered data size: {real_data_filtered.shape[0]}\")\n",
    "\n",
    "# --- Signal Shape in Range 6200-6350 from Simulation Data ---\n",
    "signal_range = (simu_data['Bc_FullDTF_M'] > 6250) & (simu_data['Bc_FullDTF_M'] < 6300)\n",
    "simu_signal_in_range = simu_data['Bc_FullDTF_M'][signal_range]\n",
    "\n",
    "# --- Calculate Scaling Factor ---\n",
    "total_sim_events = len(simu_data)  # Total number of events in the simulation\n",
    "expected_signal_events = 15  # The number of events you want to see in the histogram\n",
    "\n",
    "# Scaling factor to adjust the signal to the expected number of events\n",
    "scaling_factor = expected_signal_events / total_sim_events\n",
    "\n",
    "# --- Plot everything as histograms ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot all Bc_FullDTF Mass (real data, not filtered) as a histogram\n",
    "plt.hist(real_data['Bc_FullDTF_M'], bins=100, alpha=0.7, color='red', label=\"Original Bc_FullDTF Mass (Real Data)\", histtype='step')\n",
    "\n",
    "# Plot filtered Bc_FullDTF Mass (real data, filtered by classifier threshold) as a histogram\n",
    "plt.hist(real_data_filtered['Bc_FullDTF_M'], bins=100, alpha=0.7, color='green', label=\"Filtered Bc_FullDTF Mass (Real Data)\", histtype='step')\n",
    "\n",
    "# Plot simulation signal in Bc_FullDTF Mass range (6200-6350) as a histogram\n",
    "# Apply the scaling factor to the simulation signal histogram\n",
    "plt.hist(simu_signal_in_range, bins=100, alpha=0.7, color='purple', label=\"Simulation Signal (scaled to 15 events)\", \n",
    "         histtype='step', weights=((np.ones_like(simu_signal_in_range) * scaling_factor)+0.03))\n",
    "\n",
    "# Labels, legend, and title\n",
    "plt.xlabel(\"Bc_FullDTF Mass\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.title(\"Bc_FullDTF Mass Distribution of Real Data and Scaled Signal (15 Events)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "real_data_predictions = classif_list[i].predict_proba(X_real)[:, 1]\n",
    "real_data_filtered = real_data[real_data_predictions > 0.8]\n",
    "\n",
    "print(f\"Not-filtered data size: {real_data.shape[0]}\")\n",
    "print(f\"Filtered data size: {real_data_filtered.shape[0]}\")\n",
    "\n",
    "# --- Signal Shape in Range 6200-6350 from Simulation Data ---\n",
    "signal_range = (simu_data['Bc_FullDTF_M'] > 6250) & (simu_data['Bc_FullDTF_M'] < 6300)\n",
    "simu_signal_in_range = simu_data['Bc_FullDTF_M'][signal_range]\n",
    "\n",
    "# --- Sample 13 events from the signal distribution ---\n",
    "simu_signal_sampled = np.random.choice(simu_signal_in_range, size=13, replace=False)\n",
    "scaling_factor = 13/len(simu_signal_in_range)\n",
    "# --- Plot everything as histograms ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot all Bc_FullDTF_M Mass (real data, not filtered) as a histogram\n",
    "plt.hist(real_data['Bc_FullDTF_M'], range = (6000, 6600), bins=100, alpha=0.7, color='red', label=\"Original Bc_FullDTF_M Mass (Real Data)\", histtype='step')\n",
    "\n",
    "# Plot filtered Bc_FullDTF_M Mass (real data, filtered by classifier threshold) as a histogram\n",
    "plt.hist(real_data_filtered['Bc_FullDTF_M'], range = (6000, 6600), bins=100, alpha=0.7, color='green', label=\"Filtered Bc_FullDTF_M Mass (Real Data)\", histtype='step')\n",
    "\n",
    "# Plot exactly 13 events sampled from the simulation signal in Bc_FullDTF_M Mass range (6200-6350)\n",
    "plt.hist(simu_signal_in_range, range = (6000, 6600), bins=100, alpha=0.7, color='purple', label=\"Simulation Signal (6200-6350 range, 13 events)\", histtype='step', weights=(np.ones_like(simu_signal_in_range) * scaling_factor))\n",
    "\n",
    "# Labels, legend, and title\n",
    "plt.xlabel(\"Bc_FullDTF_M Mass\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.title(\"Bc_FullDTF_M Mass Distribution of Real Data and Signal with tighter delta_mass (143-148) (13 Events)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot all Bc_JpsiDTF Mass (simu data, not filtered) as a histogram\n",
    "plt.hist(simu_data['Bc_JpsiDTF_M'], range = (6200, 6400), bins=100, alpha=0.7, color='red', label=\"Bc_JpsiDTF_M\", histtype='step')\n",
    "\n",
    "plt.hist(simu_data['Bc_FullDTF_M'], range = (6200, 6400), bins=100, alpha=0.7, color='green', label=\"Bc_FullDTF_M\", histtype='step')\n",
    "plt.hist(simu_data['Bc_M'], range = (6200, 6400), bins=100, alpha=0.7, color='purple', label=\"Bc_M\", histtype='step')\n",
    "\n",
    "# Labels, legend, and title\n",
    "plt.xlabel(\"Bc_JpsiDTF Mass\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.title(\"Bc Mass \")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(np.unique(y_real, return_counts=True))\n",
    "\n",
    "print(np.isnan(real_data_predictions).sum())  # Check for NaNs\n",
    "print(np.unique(real_data_predictions))  # Check for constant values\n",
    "print(f\"Filtered data size: {real_data_filtered.shape[0]}\")\n",
    "\n",
    "print(f\"Filtered data size: {real_data_filtered.shape[0]}\")\n",
    "\n",
    "print(f\"y_real length: {len(y_real)}\")\n",
    "print(f\"real_data_predictions length: {len(real_data_predictions)}\")\n",
    "fpr, tpr, thresholds = roc_curve(y_real, real_data_predictions)\n",
    "if np.isnan(fpr).any() or np.isnan(tpr).any():\n",
    "    print(\"ROC curve calculation has NaN values\")\n",
    "\n",
    "\n",
    "print(X_train.columns)  # columns of the training data\n",
    "print(real_data.columns)  # columns of the real_data\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
